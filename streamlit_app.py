import platform
import sys
import os
import time
import logging
import io
import zipfile
import json
import warnings
from pathlib import Path
from typing import Dict, List, Optional, Any
from concurrent.futures import ThreadPoolExecutor, as_completed

# Third-party libs
try:
    import polars as pl
except Exception as e:
    raise ImportError("polars is required: pip install polars") from e

try:
    import duckdb
except Exception as e:
    raise ImportError("duckdb is required: pip install duckdb") from e

# pandas only used for reading extra sheets and Excel export helper
try:
    import pandas as pd
except Exception:
    pd = None  # we'll guard usage

warnings.filterwarnings('ignore')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

EXCEL_ROW_LIMIT = 1_000_000

# Streamlit fallback stub if streamlit is not installed
class _ColumnStub:
    def __init__(self, idx=0):
        self.idx = idx
    def __enter__(self): return self
    def __exit__(self, exc_type, exc, tb): return False
    def selectbox(self, *args, **kwargs):
        options = args[1] if len(args) > 1 else (args[0] if args else [])
        return options[0] if options else None
    def number_input(self, *args, **kwargs):
        return kwargs.get("value", 0.0)
    def text_input(self, *args, **kwargs):
        return ""
    def file_uploader(self, *args, **kwargs):
        return None

class _ProgressStub:
    def __init__(self, value=0, text=""):
        self.value = value
    def progress(self, value, text=None):
        self.value = value
    def empty(self):
        pass

class _SpinnerStub:
    def __init__(self, text=""):
        self.text = text
    def __enter__(self): return self
    def __exit__(self, exc_type, exc, tb): return False

class StreamlitStub:
    def __init__(self):
        self.session_state = {}
    def set_page_config(self, **kwargs):
        logger.info("set_page_config called with %s", kwargs)
    def info(self, msg): print("[INFO]", msg)
    def warning(self, msg): print("[WARN]", msg)
    def error(self, msg): print("[ERROR]", msg)
    def success(self, msg): print("[OK]", msg)
    def header(self, msg): print("=== ", msg)
    def subheader(self, msg): print("--- ", msg)
    def number_input(self, *args, **kwargs): return kwargs.get("value", 0.0)
    def checkbox(self, *args, **kwargs): return kwargs.get("value", False)
    def selectbox(self, *args, **kwargs):
        options = args[1] if len(args) > 1 else []
        idx = kwargs.get("index", 0)
        return options[idx] if options else None
    def text_input(self, *args, **kwargs): return ""
    def text_area(self, *args, **kwargs): return kwargs.get("value", "")
    def button(self, *args, **kwargs): return False
    def file_uploader(self, *args, **kwargs): return None
    def radio(self, *args, **kwargs):
        options = args[1] if len(args) > 1 else []
        return options[0] if options else None
    def multiselect(self, *args, **kwargs): return []
    def columns(self, layout):
        # return list of column stubs
        if isinstance(layout, (list, tuple)):
            return tuple(_ColumnStub(i) for i in range(len(layout)))
        return (_ColumnStub(0), _ColumnStub(1))
    def progress(self, value, text=None): return _ProgressStub(value, text)
    def spinner(self, text=""):
        return _SpinnerStub(text)
    def download_button(self, *args, **kwargs):
        # placeholder
        return None
    def dataframe(self, df, **kwargs):
        try:
            print(df.head())
        except Exception:
            print(df)
    def metric(self, label, value):
        print(f"{label}: {value}")
    # experimental rerun will be attached later

# Try to import streamlit; otherwise use stub
try:
    import streamlit as st  # type: ignore
except Exception:
    st = StreamlitStub()

# Ensure st.experimental_rerun exists
if not hasattr(st, "experimental_rerun"):
    try:
        from streamlit.runtime.scriptrunner import rerun as _streamlit_rerun  # type: ignore
        st.experimental_rerun = _streamlit_rerun  # type: ignore
    except Exception:
        def _fallback_experimental_rerun():
            st.session_state["_fallback_rerun_flag"] = not st.session_state.get(
                "_fallback_rerun_flag", False
            )
            raise RuntimeError("Requested rerun (fallback).")
        st.experimental_rerun = _fallback_experimental_rerun  # type: ignore

# Main class
class HighVolumeAutoPartsCatalog:
    def __init__(self):
        self.data_dir = Path("./auto_parts_data")
        self.data_dir.mkdir(exist_ok=True)

        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
        self.cloud_config = self.load_cloud_config()
        self.price_rules = self.load_price_rules()
        self.exclusion_rules = self.load_exclusion_rules()
        self.category_mapping = self.load_category_mapping()

        self.db_path = self.data_dir / "catalog.duckdb"
        self.conn = duckdb.connect(database=str(self.db_path))
        self.setup_database()

        try:
            st.set_page_config(
                page_title="AutoParts Catalog 10M+",
                layout="wide",
                page_icon="üöó"
            )
        except Exception:
            pass

    # --- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ ---
    def load_cloud_config(self) -> Dict[str, Any]:
        config_path = self.data_dir / "cloud_config.json"
        default_config = {
            "enabled": False,
            "provider": "s3",
            "bucket": "",
            "region": "",
            "sync_interval": 3600,
            "last_sync": 0
        }
        if config_path.exists():
            try:
                return json.loads(config_path.read_text(encoding='utf-8'))
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è cloud_config.json: {e}")
                return default_config
        else:
            config_path.write_text(json.dumps(
                default_config, indent=2, ensure_ascii=False), encoding='utf-8')
            return default_config

    def save_cloud_config(self):
        config_path = self.data_dir / "cloud_config.json"
        self.cloud_config["last_sync"] = int(time.time())
        config_path.write_text(json.dumps(
            self.cloud_config, indent=2, ensure_ascii=False), encoding='utf-8')

    def load_price_rules(self) -> Dict[str, Any]:
        price_rules_path = self.data_dir / "price_rules.json"
        default_rules = {
            "global_markup": 0.2,
            "brand_markups": {},
            "min_price": 0.0,
            "max_price": 99999.0
        }
        if price_rules_path.exists():
            try:
                return json.loads(price_rules_path.read_text(encoding='utf-8'))
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è price_rules.json: {e}")
                return default_rules
        else:
            price_rules_path.write_text(json.dumps(
                default_rules, indent=2, ensure_ascii=False), encoding='utf-8')
            return default_rules

    def save_price_rules(self):
        price_rules_path = self.data_dir / "price_rules.json"
        price_rules_path.write_text(json.dumps(
            self.price_rules, indent=2, ensure_ascii=False), encoding='utf-8')

    def load_exclusion_rules(self) -> List[str]:
        exclusion_path = self.data_dir / "exclusion_rules.txt"
        if exclusion_path.exists():
            try:
                return [line.strip() for line in exclusion_path.read_text(encoding='utf-8').splitlines() if line.strip()]
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è exclusion_rules.txt: {e}")
                return []
        else:
            content = "–ö—É–∑–æ–≤\n–°—Ç–µ–∫–ª–∞\n–ú–∞—Å–ª–∞"
            exclusion_path.write_text(content, encoding='utf-8')
            return ["–ö—É–∑–æ–≤", "–°—Ç–µ–∫–ª–∞", "–ú–∞—Å–ª–∞"]

    def save_exclusion_rules(self):
        exclusion_path = self.data_dir / "exclusion_rules.txt"
        exclusion_path.write_text(
            "\n".join(self.exclusion_rules), encoding='utf-8')

    def load_category_mapping(self) -> Dict[str, str]:
        category_path = self.data_dir / "category_mapping.txt"
        default_mapping = {
            "–†–∞–¥–∏–∞—Ç–æ—Ä": "–û—Ö–ª–∞–∂–¥–µ–Ω–∏–µ",
            "–®–∞—Ä–æ–≤–∞—è –æ–ø–æ—Ä–∞": "–ü–æ–¥–≤–µ—Å–∫–∞",
            "–§–∏–ª—å—Ç—Ä –º–∞—Å–ª—è–Ω—ã–π": "–§–∏–ª—å—Ç—Ä—ã",
            "–¢–æ—Ä–º–æ–∑–Ω—ã–µ –∫–æ–ª–æ–¥–∫–∏": "–¢–æ—Ä–º–æ–∑–∞"
        }
        if category_path.exists():
            try:
                mapping = {}
                for line in category_path.read_text(encoding='utf-8').splitlines():
                    if line.strip() and "|" in line:
                        key, value = line.split("|", 1)
                        mapping[key.strip()] = value.strip()
                return mapping
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è category_mapping.txt: {e}")
                return default_mapping
        else:
            content = "\n".join(
                [f"{k}|{v}" for k, v in default_mapping.items()])
            category_path.write_text(content, encoding='utf-8')
            return default_mapping

    def save_category_mapping(self):
        category_path = self.data_dir / "category_mapping.txt"
        content = "\n".join(
            [f"{k}|{v}" for k, v in self.category_mapping.items()])
        category_path.write_text(content, encoding='utf-8')

    # --- –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö ---
    def setup_database(self):
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS oe (
                oe_number_norm VARCHAR PRIMARY KEY,
                oe_number VARCHAR,
                name VARCHAR,
                applicability VARCHAR,
                category VARCHAR
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS parts (
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                artikul VARCHAR,
                brand VARCHAR,
                multiplicity INTEGER,
                barcode VARCHAR,
                length DOUBLE,
                width DOUBLE,
                height DOUBLE,
                weight DOUBLE,
                image_url VARCHAR,
                dimensions_str VARCHAR,
                description VARCHAR,
                PRIMARY KEY (artikul_norm, brand_norm)
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS cross_references (
                oe_number_norm VARCHAR,
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                PRIMARY KEY (oe_number_norm, artikul_norm, brand_norm)
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS prices (
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                price DOUBLE,
                currency VARCHAR DEFAULT 'RUB',
                PRIMARY KEY (artikul_norm, brand_norm)
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS metadata (
                key VARCHAR PRIMARY KEY,
                value VARCHAR
            )
        """)
        self.create_indexes()

    def create_indexes(self):
        try:
            st.info("üõ†Ô∏è –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞...")
        except Exception:
            pass
        indexes = [
            "CREATE INDEX IF NOT EXISTS idx_oe_number_norm ON oe(oe_number_norm)",
            "CREATE INDEX IF NOT EXISTS idx_parts_keys ON parts(artikul_norm, brand_norm)",
            "CREATE INDEX IF NOT EXISTS idx_cross_oe ON cross_references(oe_number_norm)",
            "CREATE INDEX IF NOT EXISTS idx_cross_artikul ON cross_references(artikul_norm, brand_norm)",
            "CREATE INDEX IF NOT EXISTS idx_prices_keys ON prices(artikul_norm, brand_norm)"
        ]
        for index_sql in indexes:
            try:
                self.conn.execute(index_sql)
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å: {e}")
        try:
            st.success("üõ†Ô∏è –ò–Ω–¥–µ–∫—Å—ã —Å–æ–∑–¥–∞–Ω—ã.")
        except Exception:
            pass

    # --- –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ –æ—á–∏—Å—Ç–∫–∞ ---
    @staticmethod
    def normalize_key(series: pl.Series) -> pl.Series:
        return (series
                .fill_null("")
                .cast(pl.Utf8)
                .str.replace_all("'", "")
                .str.replace_all(r"[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\-\s]", "")
                .str.replace_all(r"\s+", " ")
                .str.strip_chars()
                .str.to_lowercase())

    @staticmethod
    def clean_values(series: pl.Series) -> pl.Series:
        return (series
                .fill_null("")
                .cast(pl.Utf8)
                .str.replace_all("'", "")
                .str.replace_all(r"[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\-\s]", "")
                .str.replace_all(r"\s+", " ")
                .str.strip_chars())

    def determine_category_vectorized(self, name_series: pl.Series) -> pl.Series:
        name_lower = name_series.str.to_lowercase()
        categorization_expr = pl.when(pl.lit(False)).then(pl.lit(None))
        # –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞ ‚Äî –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
        for key, category in self.category_mapping.items():
            categorization_expr = categorization_expr.when(
                name_lower.str.contains(key.lower())
            ).then(pl.lit(category))
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞
        categories_map = {
            '–§–∏–ª—å—Ç—Ä': '—Ñ–∏–ª—å—Ç—Ä|filter',
            '–¢–æ—Ä–º–æ–∑–∞': '—Ç–æ—Ä–º–æ–∑|brake|–∫–æ–ª–æ–¥–∫|–¥–∏—Å–∫|—Å—É–ø–ø–æ—Ä—Ç',
            '–ü–æ–¥–≤–µ—Å–∫–∞': '–∞–º–æ—Ä—Ç–∏–∑–∞—Ç–æ—Ä|—Å—Ç–æ–π–∫|spring|–ø–æ–¥–≤–µ—Å–∫|—Ä—ã—á–∞–≥',
            '–î–≤–∏–≥–∞—Ç–µ–ª—å': '–¥–≤–∏–≥–∞—Ç–µ–ª—å|engine|—Å–≤–µ—á|–ø–æ—Ä—à–µ–Ω—å|–∫–ª–∞–ø–∞–Ω',
            '–¢—Ä–∞–Ω—Å–º–∏—Å—Å–∏—è': '—Ç—Ä–∞–Ω—Å–º–∏—Å—Å–∏—è|—Å—Ü–µ–ø–ª–µ–Ω|–∫–æ—Ä–æ–±–∫|transmission',
            '–≠–ª–µ–∫—Ç—Ä–∏–∫–∞': '–∞–∫–∫—É–º—É–ª—è—Ç–æ—Ä|–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä|—Å—Ç–∞—Ä—Ç–µ—Ä|–ø—Ä–æ–≤–æ–¥|–ª–∞–º–ø',
            '–†—É–ª–µ–≤–æ–µ': '—Ä—É–ª–µ–≤–æ–π|—Ç—è–≥–∞|–Ω–∞–∫–æ–Ω–µ—á–Ω–∏–∫|steering',
            '–í—ã–ø—É—Å–∫': '–≥–ª—É—à–∏—Ç–µ–ª—å|–∫–∞—Ç–∞–ª–∏–∑–∞—Ç–æ—Ä|–≤—ã—Ö–ª–æ–ø|exhaust',
            '–û—Ö–ª–∞–∂–¥–µ–Ω–∏–µ': '—Ä–∞–¥–∏–∞—Ç–æ—Ä|–≤–µ–Ω—Ç–∏–ª—è—Ç–æ—Ä|—Ç–µ—Ä–º–æ—Å—Ç–∞—Ç|cooling',
            '–¢–æ–ø–ª–∏–≤–æ': '—Ç–æ–ø–ª–∏–≤–Ω—ã–π|–±–µ–Ω–∑–æ–Ω–∞—Å–æ—Å|—Ñ–æ—Ä—Å—É–Ω–∫|fuel'
        }
        for category, pattern in categories_map.items():
            categorization_expr = categorization_expr.when(
                name_lower.str.contains(pattern, literal=False)
            ).then(pl.lit(category))
        return categorization_expr.otherwise(pl.lit('–†–∞–∑–Ω–æ–µ')).alias('category')

    # --- –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤ ---
    def detect_columns(self, actual_columns: List[str], expected_columns: List[str]) -> Dict[str, str]:
        column_variants = {
            'oe_number': ['oe –Ω–æ–º–µ—Ä', 'oe', '–æe', '–Ω–æ–º–µ—Ä', 'code', 'OE'],
            'artikul': ['–∞—Ä—Ç–∏–∫—É–ª', 'article', 'sku'],
            'brand': ['–±—Ä–µ–Ω–¥', 'brand', '–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å', 'manufacturer'],
            'name': ['–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', '–Ω–∞–∑–≤–∞–Ω–∏–µ', 'name', '–æ–ø–∏—Å–∞–Ω–∏–µ', 'description'],
            'applicability': ['–ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å', '–∞–≤—Ç–æ–º–æ–±–∏–ª—å', 'vehicle', 'applicability'],
            'barcode': ['—à—Ç—Ä–∏—Ö-–∫–æ–¥', 'barcode', '—à—Ç—Ä–∏—Ö–∫–æ–¥', 'ean', 'eac13'],
            'multiplicity': ['–∫—Ä–∞—Ç–Ω–æ—Å—Ç—å —à—Ç', '–∫—Ä–∞—Ç–Ω–æ—Å—Ç—å', 'multiplicity'],
            'length': ['–¥–ª–∏–Ω–∞ (—Å–º)', '–¥–ª–∏–Ω–∞', 'length', '–¥–ª–∏–Ω–Ω–∞'],
            'width': ['—à–∏—Ä–∏–Ω–∞ (—Å–º)', '—à–∏—Ä–∏–Ω–∞', 'width'],
            'height': ['–≤—ã—Å–æ—Ç–∞ (—Å–º)', '–≤—ã—Å–æ—Ç–∞', 'height'],
            'weight': ['–≤–µ—Å (–∫–≥)', '–≤–µ—Å, –∫–≥', '–≤–µ—Å', 'weight'],
            'image_url': ['—Å—Å—ã–ª–∫–∞', 'url', '–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ', 'image', '–∫–∞—Ä—Ç–∏–Ω–∫–∞'],
            'dimensions_str': ['–≤–µ—Å–æ–≥–∞–±–∞—Ä–∏—Ç—ã', '—Ä–∞–∑–º–µ—Ä—ã', 'dimensions', 'size'],
            'price': ['—Ü–µ–Ω–∞', 'price', '—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω–∞—è —Ü–µ–Ω–∞', 'retail price'],
            'currency': ['–≤–∞–ª—é—Ç–∞', 'currency']
        }
        actual_lower = {col.lower(): col for col in actual_columns}
        mapping = {}
        for expected in expected_columns:
            variants = column_variants.get(expected, [expected])
            for variant in variants:
                variant_lower = variant.lower()
                for actual_l, actual_orig in actual_lower.items():
                    if variant_lower in actual_l and actual_orig not in mapping:
                        mapping[actual_orig] = expected
                        break
        return mapping

    def import_rules_from_excel(self, file_path: str):
        """
        –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å –ø—Ä–∞–≤–∏–ª–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–π –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ª–∏—Å—Ç–æ–≤
        –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º–æ–≥–æ xlsx-—Ñ–∞–π–ª–∞. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è –ª–∏—Å—Ç—ã:
          - 'exclusions' / 'exclusion' / 'exclude'
          - 'categories' / 'category_mapping' / 'categories'
        –û–±–Ω–æ–≤–ª—è–µ—Ç self.exclusion_rules –∏ self.category_mapping –ø—Ä–∏ –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏–∏.
        """
        if pd is None:
            logger.debug("pandas not available; skipping import_rules_from_excel")
            return
        try:
            xls = pd.ExcelFile(file_path)
        except Exception as e:
            logger.debug(f"No extra sheets to read from {file_path}: {e}")
            return

        sheet_names = [s.lower() for s in xls.sheet_names]
        changed = False

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ exclusions
        for candidate in ("exclusions", "exclusion", "exclude"):
            if candidate in sheet_names:
                try:
                    df_ex = pd.read_excel(xls, sheet_name=candidate, header=0)
                    if df_ex.shape[1] >= 1:
                        col = df_ex.columns[0]
                        terms = [str(x).strip() for x in df_ex[col].astype(str).tolist() if str(x).strip() and str(x).strip().lower() not in ["nan", "none"]]
                        if terms:
                            existing = list(self.exclusion_rules or [])
                            added = 0
                            for t in terms:
                                if t not in existing:
                                    existing.append(t)
                                    added += 1
                            if added:
                                self.exclusion_rules = existing
                                try:
                                    self.save_exclusion_rules()
                                except Exception:
                                    pass
                                logger.info(f"Imported {added} exclusion terms from sheet '{candidate}'")
                                try:
                                    st.success(f"–ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –∏—Å–∫–ª—é—á–µ–Ω–∏–π: {added}")
                                except Exception:
                                    pass
                except Exception as e:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –ª–∏—Å—Ç {candidate}: {e}")
                break

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ categories
        for candidate in ("categories", "category_mapping", "category", "categories_mapping"):
            if candidate in sheet_names:
                try:
                    df_cat = pd.read_excel(xls, sheet_name=candidate, header=0)
                    if df_cat.shape[1] >= 2:
                        cols_lower = [c.lower() for c in df_cat.columns]
                        key_idx = None
                        val_idx = None
                        for i, c in enumerate(cols_lower):
                            if any(k in c for k in ("key", "name", "pattern", "–Ω–∞–∑–≤–∞–Ω–∏–µ", "–∫–ª—é—á")) and key_idx is None:
                                key_idx = i
                            if any(k in c for k in ("category", "–∫–∞—Ç–µ–≥–æ—Ä–∏—è", "value", "–∑–Ω–∞—á–µ–Ω–∏–µ")) and val_idx is None:
                                val_idx = i
                        if key_idx is None or val_idx is None:
                            key_idx, val_idx = 0, 1
                        mapping_added = 0
                        for _, row in df_cat.iterrows():
                            k = str(row.iloc[key_idx]).strip() if not pd.isna(row.iloc[key_idx]) else ""
                            v = str(row.iloc[val_idx]).strip() if not pd.isna(row.iloc[val_idx]) else ""
                            if k and v:
                                if k not in self.category_mapping or self.category_mapping.get(k) != v:
                                    self.category_mapping[k] = v
                                    mapping_added += 1
                        if mapping_added:
                            try:
                                self.save_category_mapping()
                                logger.info(f"Imported {mapping_added} category mappings from sheet '{candidate}'")
                                try:
                                    st.success(f"–ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –ø—Ä–∞–≤–∏–ª –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {mapping_added}")
                                except Exception:
                                    pass
                            except Exception as e:
                                logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è category_mapping: {e}")
                except Exception as e:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –ª–∏—Å—Ç {candidate}: {e}")
                break

    def read_and_prepare_file(self, file_path: str, file_type: str) -> pl.DataFrame:
        """
        –û–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π read_and_prepare_file: —Å–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ—Ç—Å—è –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∞–≤–∏–ª–∞ (–µ—Å–ª–∏ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç),
        –∑–∞—Ç–µ–º —á–∏—Ç–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π –ª–∏—Å—Ç (–ø–µ—Ä–≤—ã–π –ª–∏—Å—Ç) —á–µ—Ä–µ–∑ polars –∫–∞–∫ —Ä–∞–Ω—å—à–µ.
        """
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: {file_type} ({file_path})")
        # –ü–æ–ø—Ä–æ–±—É–µ–º –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∞–≤–∏–ª–∞ –∏–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ª–∏—Å—Ç–æ–≤ xlsx
        try:
            # –∏–º–ø–æ—Ä—Ç –ø—Ä–∞–≤–∏–ª –Ω–µ –∫—Ä–∏—Ç–∏—á–µ–Ω
            self.import_rules_from_excel(file_path)
        except Exception as e:
            logger.debug(f"import_rules_from_excel failed: {e}")

        try:
            if not os.path.exists(file_path):
                logger.error(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
                return pl.DataFrame()

            # polars.read_excel –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é —á–∏—Ç–∞–µ—Ç –ø–µ—Ä–≤—ã–π –ª–∏—Å—Ç ‚Äî –æ—Å—Ç–∞–≤–ª—è–µ–º –ø—Ä–µ–∂–Ω–µ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ
            df = pl.read_excel(file_path, engine='calamine')
            if df.is_empty():
                logger.warning(f"–ü—É—Å—Ç–æ–π —Ñ–∞–π–ª: {file_path}")
                return pl.DataFrame()

        except Exception as e:
            logger.exception(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {file_path}: {e}")
            return pl.DataFrame()

        # –µ—Å–ª–∏ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –ª–∏—Å—Ç–µ –µ—Å—Ç—å –∫–æ–ª–æ–Ω–∫–∏ —Å –ø—Ä–∞–≤–∏–ª–∞–º–∏ ‚Äî —Ç–∞–∫–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–µ–º –∏—Ö
        try:
            if pd is not None:
                pdf = df.to_pandas()
                # –∫–æ–ª–æ–Ω–∫–∞ —Å –∏—Å–∫–ª—é—á–µ–Ω–∏—è–º–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä 'exclude', 'exclusion', '–∏—Å–∫–ª—é—á–µ–Ω–∏–µ')
                for colname in pdf.columns:
                    if colname.lower() in ("exclude", "exclusion", "exclude_term", "–∏—Å–∫–ª—é—á–µ–Ω–∏–µ", "–∏—Å–∫–ª—é—á–µ–Ω–∏—è"):
                        terms = [str(x).strip() for x in pdf[colname].dropna().astype(str).tolist() if str(x).strip()]
                        if terms:
                            existing = list(self.exclusion_rules or [])
                            added = 0
                            for t in terms:
                                if t not in existing:
                                    existing.append(t)
                                    added += 1
                            if added:
                                self.exclusion_rules = existing
                                try:
                                    self.save_exclusion_rules()
                                except Exception:
                                    pass
                                logger.info(f"Imported {added} exclusion terms from main sheet column '{colname}'")
                        break
                # –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π (–Ω–∞–ø—Ä–∏–º–µ—Ä 'category_key' + 'category_value' –∏–ª–∏ 'key' + 'category')
                lc = [c.lower() for c in pdf.columns]
                if ("category_key" in lc and "category_value" in lc) or ("key" in lc and "category" in lc):
                    if "category_key" in lc and "category_value" in lc:
                        kcol = pdf.columns[lc.index("category_key")]
                        vcol = pdf.columns[lc.index("category_value")]
                    else:
                        kcol = pdf.columns[lc.index("key")]
                        vcol = pdf.columns[lc.index("category")]
                    mapping_added = 0
                    for _, row in pdf[[kcol, vcol]].dropna(how="all").iterrows():
                        k = str(row[kcol]).strip()
                        v = str(row[vcol]).strip()
                        if k and v:
                            if k not in self.category_mapping or self.category_mapping.get(k) != v:
                                self.category_mapping[k] = v
                                mapping_added += 1
                    if mapping_added:
                        try:
                            self.save_category_mapping()
                        except Exception:
                            pass
        except Exception:
            # –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ ‚Äî –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—ã—á–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
            pass

        schemas = {
            'oe': ['oe_number', 'artikul', 'brand', 'name', 'applicability'],
            'cross': ['oe_number', 'artikul', 'brand'],
            'barcode': ['artikul', 'brand', 'barcode', 'multiplicity'],
            'dimensions': ['artikul', 'brand', 'length', 'width', 'height', 'weight', 'dimensions_str'],
            'images': ['artikul', 'brand', 'image_url'],
            'prices': ['artikul', 'brand', 'price', 'currency']
        }
        expected_cols = schemas.get(file_type, [])
        column_mapping = self.detect_columns(df.columns, expected_cols)
        if not column_mapping:
            logger.warning(
                f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —Ñ–∞–π–ª–∞ {file_type}. –î–æ—Å—Ç—É–ø–Ω—ã–µ: {df.columns}")
            return pl.DataFrame()

        df = df.rename(column_mapping)

        for col in ['artikul', 'brand', 'oe_number']:
            if col in df.columns:
                df = df.with_columns(self.clean_values(pl.col(col)).alias(col))

        key_cols = [col for col in ['oe_number',
                                    'artikul', 'brand'] if col in df.columns]
        if key_cols:
            df = df.unique(subset=key_cols, keep='first')

        for col in ['artikul', 'brand', 'oe_number']:
            if col in df.columns:
                df = df.with_columns(self.normalize_key(
                    pl.col(col)).alias(f"{col}_norm"))

        return df

    # --- –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤ –±–∞–∑–µ ---
    def upsert_data(self, table_name: str, df: pl.DataFrame, pk: List[str]):
        if df.is_empty():
            return
        df = df.unique(keep='first')
        temp_view_name = f"temp_{table_name}_{int(time.time())}"

        # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã –≤ DuckDB
        try:
            self.conn.register(temp_view_name, df.to_arrow())
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã: {e}")
            return

        try:
            pk_list = pk
            pk_cols_csv = ", ".join(f'"{c}"' for c in pk_list)
            delete_sql = f"""
                DELETE FROM {table_name}
                WHERE ({pk_cols_csv}) IN (SELECT {pk_cols_csv} FROM {temp_view_name});
            """
            self.conn.execute(delete_sql)
            insert_sql = f"""
                INSERT INTO {table_name}
                SELECT * FROM {temp_view_name};
            """
            self.conn.execute(insert_sql)
            logger.info(
                f"–£—Å–ø–µ—à–Ω–æ upsert {len(df)} –∑–∞–ø–∏—Å–µ–π –≤ —Ç–∞–±–ª–∏—Ü—É {table_name}.")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ UPSERT –≤ {table_name}: {e}")
            try:
                st.error(
                    f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Å–∏ –≤ —Ç–∞–±–ª–∏—Ü—É {table_name}. –î–µ—Ç–∞–ª–∏ –≤ –ª–æ–≥–µ.")
            except Exception:
                pass
        finally:
            try:
                self.conn.unregister(temp_view_name)
            except Exception:
                pass

    def upsert_prices(self, price_df: pl.DataFrame):
        if price_df.is_empty():
            return

        if 'artikul' in price_df.columns and 'brand' in price_df.columns:
            price_df = price_df.with_columns([
                self.normalize_key(pl.col('artikul')).alias('artikul_norm'),
                self.normalize_key(pl.col('brand')).alias('brand_norm')
            ])

        if 'currency' not in price_df.columns:
            price_df = price_df.with_columns(pl.lit('RUB').alias('currency'))

        price_df = price_df.filter(
            (pl.col('price') >= self.price_rules['min_price']) &
            (pl.col('price') <= self.price_rules['max_price'])
        )

        self.upsert_data('prices', price_df, ['artikul_norm', 'brand_norm'])

    def process_and_load_data(self, dataframes: Dict[str, pl.DataFrame]):
        try:
            st.info("üîÑ –ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ –±–∞–∑–µ...")
        except Exception:
            pass
        steps = [s for s in ['oe', 'cross', 'parts'] if s in dataframes]
        num_steps = len(steps)
        progress_bar = st.progress(
            0, text="–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—é –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...") if hasattr(st, "progress") else None
        step_counter = 0

        if 'oe' in dataframes:
            step_counter += 1
            if progress_bar:
                progress_bar.progress(step_counter / (num_steps + 1),
                                      text=f"({step_counter}/{num_steps}) –û–±—Ä–∞–±–æ—Ç–∫–∞ OE –¥–∞–Ω–Ω—ã—Ö...")
            df = dataframes['oe'].filter(pl.col('oe_number_norm') != "")
            oe_df = df.select(['oe_number_norm', 'oe_number', 'name', 'applicability']).unique(
                subset=['oe_number_norm'], keep='first')

            if 'name' in oe_df.columns:
                oe_df = oe_df.with_columns(
                    self.determine_category_vectorized(pl.col('name')))
            else:
                oe_df = oe_df.with_columns(category=pl.lit('–†–∞–∑–Ω–æ–µ'))

            self.upsert_data('oe', oe_df, ['oe_number_norm'])

            cross_df_from_oe = df.filter(pl.col('artikul_norm') != "").select(
                ['oe_number_norm', 'artikul_norm', 'brand_norm']).unique()
            self.upsert_data('cross_references', cross_df_from_oe, [
                             'oe_number_norm', 'artikul_norm', 'brand_norm'])

        if 'cross' in dataframes:
            step_counter += 1
            if progress_bar:
                progress_bar.progress(step_counter / (num_steps + 1),
                                      text=f"({step_counter}/{num_steps}) –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫—Ä–æ—Å—Å–æ–≤...")
            df = dataframes['cross'].filter(
                (pl.col('oe_number_norm') != "") & (pl.col('artikul_norm') != ""))
            cross_df_from_cross = df.select(
                ['oe_number_norm', 'artikul_norm', 'brand_norm']).unique()
            self.upsert_data('cross_references', cross_df_from_cross, [
                             'oe_number_norm', 'artikul_norm', 'brand_norm'])

        if 'prices' in dataframes:
            price_df = dataframes['prices']
            if not price_df.is_empty():
                try:
                    st.info("üí∞ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ü–µ–Ω...")
                except Exception:
                    pass
                self.upsert_prices(price_df)
                try:
                    st.success(
                        f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–æ {len(price_df)} —Ü–µ–Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π")
                except Exception:
                    pass

        step_counter += 1
        if progress_bar:
            progress_bar.progress(step_counter / (num_steps + 1),
                                  text=f"({step_counter}/{num_steps}) –°–±–æ—Ä–∫–∞ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ –∞—Ä—Ç–∏–∫—É–ª–∞–º...")

        # –°–æ–±–∏—Ä–∞–µ–º parts –∏–∑ —Ä–∞–∑–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        parts_df = None
        file_priority = ['oe', 'barcode', 'images', 'dimensions']
        key_files = {ftype: df for ftype,
                     df in dataframes.items() if ftype in file_priority}

        if key_files:
            all_parts = pl.concat([
                df.select(['artikul', 'artikul_norm', 'brand', 'brand_norm'])
                for df in key_files.values() if 'artikul_norm' in df.columns and 'brand_norm' in df.columns
            ]).filter(pl.col('artikul_norm') != "").unique(subset=['artikul_norm', 'brand_norm'], keep='first')
            parts_df = all_parts

            for ftype in file_priority:
                if ftype not in key_files:
                    continue
                df = key_files[ftype]
                if df.is_empty() or 'artikul_norm' not in df.columns:
                    continue
                join_cols = [col for col in df.columns if col not in [
                    'artikul', 'artikul_norm', 'brand', 'brand_norm']]
                if not join_cols:
                    continue
                existing_cols = set(parts_df.columns)
                join_cols = [
                    col for col in join_cols if col not in existing_cols]
                if not join_cols:
                    continue
                df_subset = df.select(['artikul_norm', 'brand_norm'] + join_cols).unique(
                    subset=['artikul_norm', 'brand_norm'], keep='first')
                parts_df = parts_df.join(
                    df_subset, on=['artikul_norm', 'brand_norm'], how='left', coalesce=True)

        if parts_df is not None and not parts_df.is_empty():
            if 'multiplicity' not in parts_df.columns:
                parts_df = parts_df.with_columns(
                    multiplicity=pl.lit(1).cast(pl.Int32))
            else:
                parts_df = parts_df.with_columns(
                    pl.col('multiplicity').fill_null(1).cast(pl.Int32))

            for col in ['length', 'width', 'height']:
                if col not in parts_df.columns:
                    parts_df = parts_df.with_columns(
                        pl.lit(None).cast(pl.Float64).alias(col))

            if 'dimensions_str' not in parts_df.columns:
                parts_df = parts_df.with_columns(
                    dimensions_str=pl.lit(None).cast(pl.Utf8))

            parts_df = parts_df.with_columns([
                pl.col('length').cast(pl.Utf8).fill_null(
                    '').alias('_length_str'),
                pl.col('width').cast(pl.Utf8).fill_null(
                    '').alias('_width_str'),
                pl.col('height').cast(pl.Utf8).fill_null(
                    '').alias('_height_str'),
            ])

            parts_df = parts_df.with_columns(
                dimensions_str=pl.when(
                    (pl.col('dimensions_str').is_not_null()) &
                    (pl.col('dimensions_str').cast(pl.Utf8) != '')
                ).then(
                    pl.col('dimensions_str').cast(pl.Utf8)
                ).otherwise(
                    pl.concat_str([
                        pl.col('_length_str'), pl.lit('x'),
                        pl.col('_width_str'), pl.lit('x'),
                        pl.col('_height_str')
                    ], separator='')
                )
            )

            parts_df = parts_df.drop(
                ['_length_str', '_width_str', '_height_str'])

            if 'artikul' not in parts_df.columns:
                parts_df = parts_df.with_columns(artikul=pl.lit(''))
            if 'brand' not in parts_df.columns:
                parts_df = parts_df.with_columns(brand=pl.lit(''))

            parts_df = parts_df.with_columns([
                pl.col('artikul').cast(pl.Utf8).fill_null(
                    '').alias('_artikul_str'),
                pl.col('brand').cast(pl.Utf8).fill_null(
                    '').alias('_brand_str'),
                pl.col('multiplicity').cast(
                    pl.Utf8).alias('_multiplicity_str'),
            ])

            parts_df = parts_df.with_columns(
                description=pl.concat_str([
                    pl.lit('–ê—Ä—Ç–∏–∫—É–ª: '), pl.col('_artikul_str'),
                    pl.lit(', –ë—Ä–µ–Ω–¥: '), pl.col('_brand_str'),
                    pl.lit(', –ö—Ä–∞—Ç–Ω–æ—Å—Ç—å: '), pl.col(
                        '_multiplicity_str'), pl.lit(' —à—Ç.')
                ], separator='')
            )

            parts_df = parts_df.drop(
                ['_artikul_str', '_brand_str', '_multiplicity_str'])

            final_columns = [
                'artikul_norm', 'brand_norm', 'artikul', 'brand', 'multiplicity', 'barcode',
                'length', 'width', 'height', 'weight', 'image_url', 'dimensions_str', 'description'
            ]
            select_exprs = [pl.col(c) if c in parts_df.columns else pl.lit(
                None).alias(c) for c in final_columns]
            parts_df = parts_df.select(select_exprs)

            self.upsert_data('parts', parts_df, ['artikul_norm', 'brand_norm'])

        if progress_bar:
            progress_bar.progress(1.0, text="–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        time.sleep(0.2)
        if progress_bar:
            progress_bar.empty()

    # --- –≠–∫—Å–ø–æ—Ä—Ç ---
    def _get_brand_markups_sql(self) -> str:
        rows = []
        for brand, markup in self.price_rules['brand_markups'].items():
            safe_brand = brand.replace("'", "''")
            rows.append(f"SELECT '{safe_brand}' AS brand, {markup} AS markup")
        return " UNION ALL ".join(rows) if rows else "SELECT NULL AS brand, NULL AS markup LIMIT 0"

    def build_export_query(self, selected_columns=None, include_prices=True, apply_markup=True):
        description_text = (
            "–°–æ—Å—Ç–æ—è–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞: –Ω–æ–≤—ã–π (–≤ —É–ø–∞–∫–æ–≤–∫–µ). –í—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞–≤—Ç–æ–∑–∞–ø—á–∞—Å—Ç–∏ –∏ –∞–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã ‚Äî –Ω–∞–¥–µ–∂–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –≤–∞—à–µ–≥–æ –∞–≤—Ç–æ–º–æ–±–∏–ª—è. "
            "–û–±–µ—Å–ø–µ—á—å—Çe –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å, –¥–æ–ª–≥–æ–≤–µ—á–Ω–æ—Å—Ç—å –∏ –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤–∞—à–µ–≥–æ –∞–≤—Ç–æ —Å –ø–æ–º–æ—â—å—é –Ω–∞—à–µ–≥–æ —à–∏—Ä–æ–∫–æ–≥–æ –∞—Å—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –∏ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö –∞–≤—Ç–æ–∑–∞–ø—á–∞—Å—Ç–µ–π. "
            "–í—ã–±–∏—Ä–∞–π—Ç–µ —Ç–æ–ª—å–∫–æ –ª—É—á—à–µ–µ ‚Äî –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –∏ –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç –≤–µ–¥—É—â–∏—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π."
        )

        brand_markups_sql = self._get_brand_markups_sql()

        select_parts = []

        price_requested = include_prices and (not selected_columns or "–¶–µ–Ω–∞" in selected_columns or "–í–∞–ª—é—Ç–∞" in selected_columns)
        if price_requested:
            if apply_markup:
                global_markup = self.price_rules.get('global_markup', 0)
                select_parts.append(
                    f"CASE WHEN pr.price IS NOT NULL THEN pr.price * (1 + COALESCE(brm.markup, {global_markup})) ELSE pr.price END AS \"–¶–µ–Ω–∞\""
                )
            else:
                select_parts.append('pr.price AS "–¶–µ–Ω–∞"')
            select_parts.append("COALESCE(pr.currency, 'RUB') AS \"–í–∞–ª—é—Ç–∞\"")

        columns_map = [
            ("–ê—Ä—Ç–∏–∫—É–ª –±—Ä–µ–Ω–¥–∞", 'r.artikul AS "–ê—Ä—Ç–∏–∫—É–ª –±—Ä–µ–Ω–¥–∞"'),
            ("–ë—Ä–µ–Ω–¥", 'r.brand AS "–ë—Ä–µ–Ω–¥"'),
            ("–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", 'COALESCE(r.representative_name, r.analog_representative_name) AS "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ"'),
            ("–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å", 'COALESCE(r.representative_applicability, r.analog_representative_applicability) AS "–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å"'),
            ("–û–ø–∏—Å–∞–Ω–∏–µ", 'CONCAT(COALESCE(r.description, \'\'), dt.text) AS "–û–ø–∏—Å–∞–Ω–∏–µ"'),
            ("–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞", 'COALESCE(r.representative_category, r.analog_representative_category) AS "–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞"'),
            ("–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å", 'r.multiplicity AS "–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å"'),
            ("–î–ª–∏–Ω–Ω–∞", 'COALESCE(r.length, r.analog_length) AS "–î–ª–∏–Ω–Ω–∞"'),
            ("–®–∏—Ä–∏–Ω–∞", 'COALESCE(r.width, r.analog_width) AS "–®–∏—Ä–∏–Ω–∞"'),
            ("–í—ã—Å–æ—Ç–∞", 'COALESCE(r.height, r.analog_height) AS "–í—ã—Å–æ—Ç–∞"'),
            ("–í–µ—Å", 'COALESCE(r.weight, r.analog_weight) AS "–í–µ—Å"'),
            ("–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞", """
                COALESCE(
                    CASE
                        WHEN r.dimensions_str IS NULL OR r.dimensions_str = '' OR UPPER(TRIM(r.dimensions_str)) = 'XX'
                        THEN NULL
                        ELSE r.dimensions_str
                    END,
                    r.analog_dimensions_str
                ) AS "–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞"
            """),
            ("OE –Ω–æ–º–µ—Ä", 'r.oe_list AS "OE –Ω–æ–º–µ—Ä"'),
            ("–∞–Ω–∞–ª–æ–≥–∏", 'r.analog_list AS "–∞–Ω–∞–ª–æ–≥–∏"'),
            ("–°—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", 'r.image_url AS "–°—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"')
        ]

        for name, expr in columns_map:
            if not selected_columns or name in selected_columns:
                select_parts.append(expr.strip())

        if not select_parts:
            select_parts = ['r.artikul AS "–ê—Ä—Ç–∏–∫—É–ª –±—Ä–µ–Ω–¥–∞"', 'r.brand AS "–ë—Ä–µ–Ω–¥"']

        select_clause = ",\n        ".join(select_parts)

        ctes = f"""
        WITH DescriptionTemplate AS (
            SELECT CHR(10) || CHR(10) || $${description_text}$$ AS text
        ),
        BrandMarkups AS (
            SELECT brand, markup FROM (
                {brand_markups_sql}
            ) AS tmp
        ),
        PartDetails AS (
            SELECT 
                cr.artikul_norm, 
                cr.brand_norm,
                STRING_AGG(
                    DISTINCT regexp_replace(
                        regexp_replace(o.oe_number, '''', ''), 
                        '[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\\-\\s]', '', 'g'
                    ), ', '
                ) AS oe_list,
                ANY_VALUE(o.name) AS representative_name,
                ANY_VALUE(o.applicability) AS representative_applicability,
                ANY_VALUE(o.category) AS representative_category
            FROM cross_references cr
            LEFT JOIN oe o ON cr.oe_number_norm = o.oe_number_norm
            GROUP BY cr.artikul_norm, cr.brand_norm
        ),
        AllAnalogs AS (
            SELECT 
                cr1.artikul_norm, 
                cr1.brand_norm,
                STRING_AGG(
                    DISTINCT regexp_replace(
                        regexp_replace(p2.artikul, '''', ''), 
                        '[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\\-\\s]', '', 'g'
                    ), ', '
                ) AS analog_list
            FROM cross_references cr1
            JOIN cross_references cr2 ON cr1.oe_number_norm = cr2.oe_number_norm
            JOIN parts p2 ON cr2.artikul_norm = p2.artikul_norm AND cr2.brand_norm = p2.brand_norm
            WHERE (cr1.artikul_norm != p2.artikul_norm OR cr1.brand_norm != p2.brand_norm)
            GROUP BY cr1.artikul_norm, cr1.brand_norm
        ),
        InitialOENumbers AS (
            SELECT DISTINCT p.artikul_norm, p.brand_norm, cr.oe_number_norm
            FROM parts p
            LEFT JOIN cross_references cr ON p.artikul_norm = cr.artikul_norm AND p.brand_norm = cr.brand_norm
            WHERE cr.oe_number_norm IS NOT NULL
        ),
        Level1Analogs AS (
            SELECT DISTINCT 
                i.artikul_norm AS source_artikul_norm, 
                i.brand_norm AS source_brand_norm,
                cr2.artikul_norm AS related_artikul_norm, 
                cr2.brand_norm AS related_brand_norm
            FROM InitialOENumbers i
            JOIN cross_references cr2 ON i.oe_number_norm = cr2.oe_number_norm
            WHERE NOT (i.artikul_norm = cr2.artikul_norm AND i.brand_norm = cr2.brand_norm)
        ),
        Level1OENumbers AS (
            SELECT DISTINCT 
                l1.source_artikul_norm, 
                l1.source_brand_norm, 
                cr3.oe_number_norm
            FROM Level1Analogs l1
            JOIN cross_references cr3 ON l1.related_artikul_norm = cr3.artikul_norm AND l1.related_brand_norm = cr3.brand_norm
            WHERE NOT EXISTS (
                SELECT 1 FROM InitialOENumbers i
                WHERE i.artikul_norm = l1.source_artikul_norm 
                  AND i.brand_norm = l1.source_brand_norm 
                  AND i.oe_number_norm = cr3.oe_number_norm
            )
        ),
        Level2Analogs AS (
            SELECT DISTINCT 
                loe.source_artikul_norm, 
                loe.source_brand_norm,
                cr4.artikul_norm AS related_artikul_norm, 
                cr4.brand_norm AS related_brand_norm
            FROM Level1OENumbers loe
            JOIN cross_references cr4 ON loe.oe_number_norm = cr4.oe_number_norm
            WHERE NOT (loe.source_artikul_norm = cr4.artikul_norm AND loe.source_brand_norm = cr4.brand_norm)
        ),
        AllRelatedParts AS (
            SELECT source_artikul_norm, source_brand_norm, related_artikul_norm, related_brand_norm
            FROM Level1Analogs
            UNION
            SELECT source_artikul_norm, source_brand_norm, related_artikul_norm, related_brand_norm
            FROM Level2Analogs
        ),
        AggregatedAnalogData AS (
            SELECT 
                arp.source_artikul_norm AS artikul_norm,
                arp.source_brand_norm AS brand_norm,
                MAX(CASE WHEN p2.length IS NOT NULL THEN p2.length ELSE NULL END) AS length,
                MAX(CASE WHEN p2.width IS NOT NULL THEN p2.width ELSE NULL END) AS width,
                MAX(CASE WHEN p2.height IS NOT NULL THEN p2.height ELSE NULL END) AS height,
                MAX(CASE WHEN p2.weight IS NOT NULL THEN p2.weight ELSE NULL END) AS weight,
                ANY_VALUE(
                    CASE 
                        WHEN p2.dimensions_str IS NOT NULL AND p2.dimensions_str != '' AND UPPER(TRIM(p2.dimensions_str)) != 'XX'
                        THEN p2.dimensions_str
                        ELSE NULL
                    END
                ) AS dimensions_str,
                ANY_VALUE(
                    CASE 
                        WHEN pd2.representative_name IS NOT NULL AND pd2.representative_name != '' 
                        THEN pd2.representative_name 
                        ELSE NULL
                    END
                ) AS representative_name,
                ANY_VALUE(
                    CASE 
                        WHEN pd2.representative_applicability IS NOT NULL AND pd2.representative_applicability != ''
                        THEN pd2.representative_applicability
                        ELSE NULL
                    END
                ) AS representative_applicability,
                ANY_VALUE(
                    CASE 
                        WHEN pd2.representative_category IS NOT NULL AND pd2.representative_category != ''
                        THEN pd2.representative_category
                        ELSE NULL
                    END
                ) AS representative_category
            FROM AllRelatedParts arp
            JOIN parts p2 ON arp.related_artikul_norm = p2.artikul_norm AND arp.related_brand_norm = p2.brand_norm
            LEFT JOIN PartDetails pd2 ON p2.artikul_norm = pd2.artikul_norm AND p2.brand_norm = pd2.brand_norm
            GROUP BY arp.source_artikul_norm, arp.source_brand_norm
        ),
        RankedData AS (
            SELECT 
                p.artikul_norm,
                p.brand_norm,
                p.artikul,
                p.brand,
                p.description,
                p.multiplicity,
                p.length,
                p.width,
                p.height,
                p.weight,
                p.dimensions_str,
                p.image_url,
                pd.representative_name,
                pd.representative_applicability,
                pd.representative_category,
                pd.oe_list,
                aa.analog_list,
                p_analog.length AS analog_length,
                p_analog.width AS analog_width,
                p_analog.height AS analog_height,
                p_analog.weight AS analog_weight,
                p_analog.dimensions_str AS analog_dimensions_str,
                p_analog.representative_name AS analog_representative_name,
                p_analog.representative_applicability AS analog_representative_applicability,
                p_analog.representative_category AS analog_representative_category,
                ROW_NUMBER() OVER (
                    PARTITION BY p.artikul_norm, p.brand_norm 
                    ORDER BY pd.representative_name DESC NULLS LAST, pd.oe_list DESC NULLS LAST
                ) AS rn
            FROM parts p
            LEFT JOIN PartDetails pd ON p.artikul_norm = pd.artikul_norm AND p.brand_norm = pd.brand_norm
            LEFT JOIN AllAnalogs aa ON p.artikul_norm = aa.artikul_norm AND p.brand_norm = aa.brand_norm
            LEFT JOIN AggregatedAnalogData p_analog ON p.artikul_norm = p_analog.artikul_norm AND p.brand_norm = p_analog.brand_norm
        )
        """

        price_join = """
        LEFT JOIN prices pr ON r.artikul_norm = pr.artikul_norm AND r.brand_norm = pr.brand_norm
        LEFT JOIN BrandMarkups brm ON r.brand = brm.brand
        """ if include_prices else ""

        query = f"""
        {ctes}
        SELECT
            {select_clause}
        FROM RankedData r
        CROSS JOIN DescriptionTemplate dt
        {price_join}
        WHERE r.rn = 1
        ORDER BY r.brand, r.artikul
        """

        return "\n".join([line.rstrip() for line in query.strip().splitlines()])

    def export_to_csv_optimized(self, output_path: str, selected_columns: Optional[List[str]] = None, include_prices: bool = True, apply_markup: bool = True) -> bool:
        total = self.conn.execute(
            "SELECT count(*) FROM (SELECT DISTINCT artikul_norm, brand_norm FROM parts)").fetchone()[0]
        if total == 0:
            st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")
            return False
        st.info(f"üì§ –≠–∫—Å–ø–æ—Ä—Ç {total} –∑–∞–ø–∏—Å–µ–π –≤ CSV...")
        try:
            query = self.build_export_query(
                selected_columns, include_prices, apply_markup)
            logger.info(f"Executing export query: {query}")
            df = self.conn.execute(query).pl()
            import pandas as _pd
            pdf = df.to_pandas()

            dimension_cols = ["–î–ª–∏–Ω–Ω–∞", "–®–∏—Ä–∏–Ω–∞",
                              "–í—ã—Å–æ—Ç–∞", "–í–µ—Å", "–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞"]
            for col in dimension_cols:
                if col in pdf.columns:
                    pdf[col] = pdf[col].astype(str).replace({'nan': ''})

            output_dir = Path("auto_parts_data")
            output_dir.mkdir(parents=True, exist_ok=True)

            buf = io.StringIO()
            pdf.to_csv(buf, sep=';', index=False)
            with open(output_path, "wb") as f:
                f.write(b'\xef\xbb\xbf')
                f.write(buf.getvalue().encode('utf-8'))
            size_mb = os.path.getsize(output_path) / (1024 * 1024)
            st.success(
                f"–î–∞–Ω–Ω—ã–µ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã: {output_path} ({size_mb:.1f} –ú–ë)")
            return True
        except Exception as e:
            logger.exception("–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ CSV")
            try:
                st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ –≤ CSV: {str(e)}")
            except Exception:
                pass
            return False

    def export_to_excel_optimized(self, output_path: str, selected_columns: Optional[List[str]] = None, include_prices: bool = True, apply_markup: bool = True) -> bool:
        total = self.conn.execute(
            "SELECT COUNT(*) FROM (SELECT DISTINCT artikul_norm, brand_norm FROM parts)").fetchone()[0]
        if total == 0:
            st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")
            return False
        import pandas as _pd
        query = self.build_export_query(
            selected_columns, include_prices, apply_markup)
        df = _pd.read_sql(query, self.conn)
        for col in ["–î–ª–∏–Ω–Ω–∞", "–®–∏—Ä–∏–Ω–∞", "–í—ã—Å–æ—Ç–∞", "–í–µ—Å", "–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞"]:
            if col in df.columns:
                df[col] = df[col].astype(str).replace(
                    {r'^nan$': ''}, regex=True)
        if len(df) <= EXCEL_ROW_LIMIT:
            with _pd.ExcelWriter(output_path, engine='openpyxl') as writer:
                df.to_excel(writer, index=False)
        else:
            sheets = (len(df) // EXCEL_ROW_LIMIT) + 1
            with _pd.ExcelWriter(output_path, engine='openpyxl') as writer:
                for i in range(sheets):
                    df.iloc[i*EXCEL_ROW_LIMIT:(i+1)*EXCEL_ROW_LIMIT].to_excel(
                        writer, index=False, sheet_name=f"–î–∞–Ω–Ω—ã–µ_{i+1}")
        return True

    def export_to_parquet(self, output_path: str, selected_columns: Optional[List[str]] = None, include_prices: bool = True, apply_markup: bool = True) -> bool:
        try:
            query = self.build_export_query(
                selected_columns, include_prices, apply_markup)
            df = self.conn.execute(query).pl()
            df.write_parquet(output_path)
            return True
        except Exception as e:
            logger.exception("–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ Parquet")
            try:
                st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ –≤ Parquet: {str(e)}")
            except Exception:
                pass
            return False

    # --- –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã–º–∏ ---
    def delete_by_brand(self, brand_norm: str) -> int:
        try:
            count_result = self.conn.execute(
                "SELECT COUNT(*) FROM parts WHERE brand_norm = ?", [brand_norm]).fetchone()
            deleted_count = count_result[0] if count_result else 0
            if deleted_count == 0:
                logger.info(f"No records found for brand: {brand_norm}")
                return 0
            self.conn.execute(
                "DELETE FROM parts WHERE brand_norm = ?", [brand_norm])
            self.conn.execute(
                "DELETE FROM cross_references WHERE (artikul_norm, brand_norm) NOT IN (SELECT DISTINCT artikul_norm, brand_norm FROM parts)")
            return deleted_count
        except Exception as e:
            logger.error(f"Error deleting by brand {brand_norm}: {e}")
            raise

    def delete_by_artikul(self, artikul_norm: str) -> int:
        try:
            count_result = self.conn.execute(
                "SELECT COUNT(*) FROM parts WHERE artikul_norm = ?", [artikul_norm]).fetchone()
            deleted_count = count_result[0] if count_result else 0
            if deleted_count == 0:
                logger.info(f"No records found for artikul: {artikul_norm}")
                return 0
            self.conn.execute(
                "DELETE FROM parts WHERE artikul_norm = ?", [artikul_norm])
            self.conn.execute(
                "DELETE FROM cross_references WHERE (artikul_norm, brand_norm) NOT IN (SELECT DISTINCT artikul_norm, brand_norm FROM parts)")
            return deleted_count
        except Exception as e:
            logger.error(f"Error deleting by artikul {artikul_norm}: {e}")
            raise

    # --- –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã/–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã (–∫–æ–Ω—Å–æ–ª—å–Ω—ã–µ –∑–∞–≥–ª—É—à–∫–∏ –¥–ª—è —Å—Ä–µ–¥—ã –±–µ–∑ Streamlit) ---
    def show_statistics(self):
        st.header("üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
        stats = {}
        try:
            stats['parts'] = self.conn.execute(
                "SELECT COUNT(*) FROM parts").fetchone()[0]
            stats['oe'] = self.conn.execute(
                "SELECT COUNT(*) FROM oe").fetchone()[0]
            stats['cross'] = self.conn.execute(
                "SELECT COUNT(*) FROM cross_references").fetchone()[0]
            stats['prices'] = self.conn.execute(
                "SELECT COUNT(*) FROM prices").fetchone()[0]
            stats['brands'] = self.conn.execute(
                "SELECT COUNT(DISTINCT brand) FROM parts").fetchone()[0]
            stats['unique_parts'] = self.conn.execute(
                "SELECT COUNT(*) FROM (SELECT DISTINCT artikul_norm, brand_norm FROM parts)").fetchone()[0]
            avg_price = self.conn.execute(
                "SELECT AVG(price) FROM prices").fetchone()[0]
            stats['avg_price'] = round(avg_price, 2) if avg_price else 0
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
            return
        try:
            col1, col2, col3 = st.columns(3)
            col1.metric("–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤", f"{stats['unique_parts']:,}")
            col2.metric("–ë—Ä–µ–Ω–¥–æ–≤", f"{stats['brands']:,}")
            col3.metric("–°—Ä–µ–¥–Ω—è—è —Ü–µ–Ω–∞", f"{stats['avg_price']} ‚ÇΩ")
        except Exception:
            print("Unique:", stats.get('unique_parts'), "Brands:", stats.get('brands'), "Avg price:", stats.get('avg_price'))

        try:
            top_brands = self.conn.execute(
                "SELECT brand, COUNT(*) as cnt FROM parts GROUP BY brand ORDER BY cnt DESC LIMIT 10").pl()
            st.subheader("–¢–æ–ø 10 –±—Ä–µ–Ω–¥–æ–≤")
            st.dataframe(top_brands.to_pandas())
        except Exception:
            pass

    def merge_all_data_parallel(self, file_paths: Dict[str, str], max_workers: int = 4) -> Dict[str, pl.DataFrame]:
        results = {}
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {}
            for key, path in file_paths.items():
                if path and os.path.exists(path):
                    futures[executor.submit(
                        self.read_and_prepare_file, path, key)] = key
            for fut in as_completed(futures):
                key = futures[fut]
                try:
                    df = fut.result()
                    if not df.is_empty():
                        results[key] = df
                        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω {key}")
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {key}: {e}")
        return results

# –ï—Å–ª–∏ —ç—Ç–æ—Ç –º–æ–¥—É–ª—å –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –Ω–∞–ø—Ä—è–º—É—é ‚Äî –ø—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
def main():
    catalog = HighVolumeAutoPartsCatalog()
    print("Catalog initialized. Use catalog.process_and_load_data / export methods in scripts.")
    # –ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É (–±–µ–∑ streamlit –±—É–¥–µ—Ç –≤—ã–≤–æ–¥ –≤ –∫–æ–Ω—Å–æ–ª—å)
    catalog.show_statistics()

if __name__ == "__main__":
    main()
